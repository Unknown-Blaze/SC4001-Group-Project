# Fine-Tuning vs LoRA: A Comparative Study for Sentiment Analysis
## SC4001: Neural Networks and Deep Learning - Academic Project

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## üìã Table of Contents

- [Overview](#overview)
- [Research Motivation](#research-motivation)
- [Project Structure](#project-structure)
- [Methods Compared](#methods-compared)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Detailed Usage](#detailed-usage)
- [Experimental Results](#experimental-results)
- [For Academic Submission](#for-academic-submission)
- [Citation](#citation)
- [Contributors](#contributors)

---

## üéØ Overview

This repository contains the implementation and experimental framework for comparing **traditional fine-tuning** versus **Low-Rank Adaptation (LoRA)** techniques for sentiment analysis tasks. This work is part of the SC4001: Neural Networks and Deep Learning course project.

### Key Contributions

1. **Comprehensive Comparison**: Systematic evaluation of fine-tuning vs. LoRA across multiple model architectures
2. **Domain Adaptation**: Cross-domain evaluation (IMDB + Yelp ‚Üí Amazon)
3. **Reproducible Pipeline**: Complete end-to-end workflow with K-Fold CV and Optuna optimization
4. **Multiple Architectures**: TF-IDF+FFNN, E5 Embeddings, and BERT-based models

### Research Question

> **How does parameter-efficient fine-tuning (LoRA) compare to traditional full fine-tuning in terms of performance, efficiency, and generalization for sentiment analysis tasks?**

---

## üî¨ Research Motivation

### Why Compare Fine-Tuning vs. LoRA?

**Traditional Fine-Tuning**:
- ‚úÖ Updates all model parameters
- ‚úÖ Often achieves highest accuracy
- ‚ùå Computationally expensive
- ‚ùå Large storage requirements
- ‚ùå Risk of catastrophic forgetting

**LoRA (Low-Rank Adaptation)**:
- ‚úÖ Parameter-efficient (< 1% parameters updated)
- ‚úÖ Faster training
- ‚úÖ Lower memory footprint
- ‚úÖ Maintains pre-trained knowledge
- ‚ùì Performance trade-offs?

### Dataset: Multi-Domain Sentiment Analysis

- **Training**: IMDB (movie reviews) + Yelp (restaurant reviews)
- **In-Domain Testing**: IMDB + Yelp test sets
- **Cross-Domain Testing**: Amazon product reviews (domain adaptation)

**Total Samples**: 
- Training: 48,000 samples (24k per domain)
- Validation: 12,000 samples (6k per domain)
- Test: 12,000 samples (6k per domain)

---

## üìÅ Project Structure

```
export_package/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ requirements.txt                   # Master dependencies list
‚îÇ
‚îú‚îÄ‚îÄ data/                              # Shared data directory
‚îÇ   ‚îî‚îÄ‚îÄ processed/                     # Preprocessed datasets
‚îÇ       ‚îú‚îÄ‚îÄ train.json                 # Training data (IMDB + Yelp)
‚îÇ       ‚îú‚îÄ‚îÄ eval.json                  # Validation data
‚îÇ       ‚îú‚îÄ‚îÄ amazon_eval.json           # Cross-domain test data
‚îÇ       ‚îî‚îÄ‚îÄ dataset_summary.json       # Dataset statistics
‚îÇ
‚îú‚îÄ‚îÄ fine_tuning/                       # Method A: Traditional Fine-Tuning
‚îÇ   ‚îú‚îÄ‚îÄ first.ipynb                    # [1] Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ train_all_models.py            # [2] Main training script (CLI)
‚îÇ   ‚îú‚îÄ‚îÄ third.ipynb                    # [3] Evaluation & visualization
‚îÇ   ‚îú‚îÄ‚îÄ train_bert_only.py             # Standalone BERT trainer
‚îÇ   ‚îú‚îÄ‚îÄ run_training.sh                # Convenience wrapper script
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING_README.md             # Detailed fine-tuning docs
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt               # Fine-tuning dependencies
‚îÇ   ‚îî‚îÄ‚îÄ models/                        # Trained models output
‚îÇ       ‚îú‚îÄ‚îÄ tfidf_ffnn/
‚îÇ       ‚îú‚îÄ‚îÄ e5_classifier/
‚îÇ       ‚îî‚îÄ‚îÄ bert_finetuned/
‚îÇ
‚îî‚îÄ‚îÄ lora/                              # Method B: LoRA (Parameter-Efficient)
    ‚îú‚îÄ‚îÄ train_method_b_e5.py           # LoRA training with E5 + Optuna
    ‚îú‚îÄ‚îÄ models_e5.py                   # E5 model architectures
    ‚îú‚îÄ‚îÄ metrics_utils.py               # Evaluation utilities
    ‚îú‚îÄ‚îÄ inference.py                   # Inference script
    ‚îî‚îÄ‚îÄ outputs/                       # LoRA checkpoints & results
        ‚îú‚îÄ‚îÄ domain_adapter_*.pt        # Domain-specific adapters
        ‚îú‚îÄ‚îÄ fusion_model.pt            # Fusion layer weights
        ‚îî‚îÄ‚îÄ metrics/                   # Training metrics
```

---

## üîç Methods Compared

### Method A: Traditional Fine-Tuning

Three model variants trained with full parameter updates:

#### 1. **TF-IDF + FFNN** (Baseline)
- **Architecture**: TF-IDF vectorizer (10k features) + 2-layer MLP
- **Parameters**: ~2.5M trainable
- **Training**: K-Fold CV (3-fold) + Optuna (15 trials)
- **Purpose**: Classical baseline for comparison

#### 2. **E5 Embedding Classifier** (Frozen Embeddings)
- **Base Model**: `intfloat/e5-small-v2` (33.4M params)
- **Architecture**: Frozen E5 + MLP classifier head
- **Parameters**: ~512k trainable (classifier only)
- **Training**: K-Fold CV (3-fold) + Optuna (12 trials)
- **Purpose**: Transfer learning baseline

#### 3. **BERT Fine-Tuning** (Full Fine-Tuning)
- **Base Model**: `bert-base-uncased` (110M params)
- **Architecture**: BERT encoder + classification head
- **Parameters**: ~109.5M trainable (all parameters)
- **Training**: K-Fold CV (2-fold) + Optuna (8 trials)
- **Purpose**: State-of-the-art full fine-tuning

**Key Characteristics**:
- All model parameters updated during training
- Highest performance potential
- Computational cost: HIGH
- Storage: ~440MB per model checkpoint

---

### Method B: LoRA (Low-Rank Adaptation)

Parameter-efficient fine-tuning with domain adaptation:

#### Architecture: E5 + LoRA + Domain Fusion

```
Input Text ‚Üí E5 Encoder (with LoRA) ‚Üí Domain Adapters ‚Üí Fusion ‚Üí Classification
                  ‚Üì                           ‚Üì               ‚Üì
          LoRA Layers (r=8)     IMDB Adapter + Yelp Adapter  ‚Üí Weighted Combination
```

**Components**:

1. **E5 Backbone with LoRA**
   - Base Model: `intfloat/e5-small-v2` (33.4M params)
   - LoRA rank: r=8, alpha=16
   - Trainable LoRA parameters: ~295k (< 1% of base model)
   - Frozen parameters: 33.1M

2. **Domain-Specific Adapters**
   - IMDB adapter: Trained on movie reviews
   - Yelp adapter: Trained on restaurant reviews
   - Each adapter: Independent LoRA fine-tuning on domain data
   - Training: K-Fold CV (3-fold) + Optuna (10 trials per domain)

3. **Fusion Layer**
   - Learned weighted combination of domain adapters
   - Attention-based fusion mechanism
   - Training: Combined IMDB + Yelp data
   - Optimizes cross-domain generalization

**Key Characteristics**:
- Only ~295k parameters updated (0.88% of E5 base model)
- Modular: Domain adapters can be combined/swapped
- Computational cost: LOW (4-6x faster than full fine-tuning)
- Storage: ~1.2MB per LoRA adapter (vs. ~440MB for full model)

**LoRA Hyperparameters**:
```python
{
    "lora_r": 8,              # Rank of low-rank matrices
    "lora_alpha": 16,         # Scaling factor (alpha/r = 2.0)
    "lora_dropout": 0.1,      # Dropout for LoRA layers
    "target_modules": ["query", "value"],  # Which attention layers to adapt
    "learning_rate": 1e-4,    # Optimized via Optuna
    "batch_size": 32,         # Optimized via Optuna
    "num_epochs": 10          # With early stopping
}
```

---

## üíª Installation

### System Requirements

- **OS**: Linux (Ubuntu 20.04+), macOS, or Windows with WSL
- **Python**: 3.8 or higher
- **GPU**: CUDA-capable GPU recommended (12GB+ VRAM for BERT)
- **RAM**: 16GB minimum (32GB recommended for BERT training)
- **Storage**: ~5GB for datasets and models

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/finetuning-vs-lora.git
cd finetuning-vs-lora/export_package
```

### Step 2: Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n sentiment_analysis python=3.8
conda activate sentiment_analysis

# Or using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

#### Option A: Master Installation (All Methods)
```bash
pip install -r requirements.txt
```

#### Option B: Method-Specific Installation

**For Fine-Tuning Only:**
```bash
cd fine_tuning
pip install -r requirements.txt
```

**For LoRA Only:**
```bash
pip install torch transformers datasets peft optuna scikit-learn tqdm
```

### Step 4: Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"
```

---

## üöÄ Quick Start

### Complete Pipeline (All Methods)

```bash
# Step 1: Preprocess data (run once)
cd fine_tuning
jupyter notebook first.ipynb
# Run all cells to create ../data/processed/

# Step 2: Train fine-tuning models
python train_all_models.py 2>&1 | tee training_log.txt

# Step 3: Train LoRA models
cd ../lora
python train_method_b_e5.py 2>&1 | tee lora_training_log.txt

# Step 4: Evaluate and compare
cd ../fine_tuning
jupyter notebook third.ipynb
# Run all cells for visualizations and comparisons
```

### Quick Test Mode (Faster)

```bash
# Fine-tuning with reduced trials
cd fine_tuning
python train_all_models.py --n-trials 3 --models bert

# LoRA with reduced trials
cd ../lora
python train_method_b_e5.py --n_trials 3
```

---

## üìö Detailed Usage

### Method A: Fine-Tuning Pipeline

#### Step 1: Data Preprocessing

```bash
cd fine_tuning
jupyter notebook first.ipynb
```

**What it does**:
- Downloads IMDB, Yelp, and Amazon datasets
- Balances classes (50/50 positive/negative)
- Creates train/val/test splits
- Saves to `../data/processed/`

**Output**:
```
data/processed/
‚îú‚îÄ‚îÄ train.json           # 48,000 samples (IMDB + Yelp)
‚îú‚îÄ‚îÄ eval.json            # 12,000 samples (validation)
‚îú‚îÄ‚îÄ amazon_eval.json     # 12,000 samples (cross-domain test)
‚îú‚îÄ‚îÄ imdb_eval.json       # 6,000 samples (in-domain test)
‚îú‚îÄ‚îÄ yelp_eval.json       # 6,000 samples (in-domain test)
‚îî‚îÄ‚îÄ dataset_summary.json # Statistics
```

#### Step 2: Train Models

**All models** (recommended for paper):
```bash
python train_all_models.py
```

**Specific models**:
```bash
# TF-IDF only
python train_all_models.py --models tfidf

# BERT only (faster alternative)
python train_bert_only.py

# E5 and BERT
python train_all_models.py --models e5 bert
```

**Custom hyperparameter search**:
```bash
# More thorough search (longer training)
python train_all_models.py --n-trials 20

# Skip optimization (use saved hyperparameters)
python train_all_models.py --skip-optuna
```

**Progress Tracking**:
- Real-time progress bars for each model
- Validation metrics after each epoch
- Best model saved automatically
- Full training logs saved to `training_YYYYMMDD_HHMMSS.log`

**Expected Training Times** (with GPU):
| Model | Trials | Folds | Time |
|-------|--------|-------|------|
| TF-IDF | 15 | 3 | 20-40 min |
| E5 | 12 | 3 | 15-30 min |
| BERT | 8 | 2 | 1-3 hours |

#### Step 3: Evaluation

```bash
jupyter notebook third.ipynb
```

**What it does**:
- Loads all trained models
- Evaluates on in-domain test sets
- **Cross-domain evaluation** on Amazon
- Generates comparison plots and tables
- Saves results to `outputs/evaluation/`

**Generated Outputs**:
- Performance comparison tables (CSV)
- Confusion matrices (PNG)
- ROC curves (PNG)
- Domain adaptation analysis (JSON)
- F1-score heatmaps (PNG)

---

### Method B: LoRA Training

#### Training Domain Adapters + Fusion Model

```bash
cd lora
python train_method_b_e5.py
```

**Training Process**:

1. **Stage 1: IMDB Domain Adapter**
   - K-Fold CV (3-fold) on IMDB data
   - Optuna optimization (10 trials)
   - Best adapter saved to `outputs/domain_adapter_imdb.pt`

2. **Stage 2: Yelp Domain Adapter**
   - K-Fold CV (3-fold) on Yelp data
   - Optuna optimization (10 trials)
   - Best adapter saved to `outputs/domain_adapter_yelp.pt`

3. **Stage 3: Fusion Training**
   - Combines both domain adapters
   - K-Fold CV (2-fold) on combined data
   - Optuna optimization (10 trials)
   - Fusion layer saved to `outputs/fusion_model.pt`

**Command-Line Options**:
```bash
# Custom trials
python train_method_b_e5.py --n_trials 15

# Custom output directory
python train_method_b_e5.py --output_dir custom_outputs

# Skip specific stages (if already trained)
python train_method_b_e5.py --skip_imdb  # Use existing IMDB adapter
python train_method_b_e5.py --skip_yelp  # Use existing Yelp adapter
```

**Expected Training Times** (with GPU):
| Stage | Trials | Folds | Time |
|-------|--------|-------|------|
| IMDB Adapter | 10 | 3 | 30-45 min |
| Yelp Adapter | 10 | 3 | 30-45 min |
| Fusion | 10 | 2 | 20-30 min |
| **Total** | - | - | **~1.5-2 hours** |

#### Inference with LoRA

```bash
# Single prediction
python inference.py --text "This movie was amazing!" --domain imdb

# Batch predictions from file
python inference.py --input_file test_samples.txt --output predictions.json

# Use specific adapter
python inference.py --adapter outputs/domain_adapter_imdb.pt --text "Great film!"
```

---

## üìä Experimental Results

### Performance Summary

*(Note: Update these with your actual results)*

#### In-Domain Performance (IMDB + Yelp Test Sets)

| Model | Accuracy | F1 Score | Precision | Recall | Parameters (M) | Training Time |
|-------|----------|----------|-----------|--------|----------------|---------------|
| **Fine-Tuning** | | | | | | |
| TF-IDF + FFNN | 88.5% | 0.883 | 0.886 | 0.880 | 2.5 | 30 min |
| E5 Classifier | 90.2% | 0.901 | 0.903 | 0.899 | 0.5 | 25 min |
| BERT (Full) | **93.4%** | **0.934** | **0.935** | **0.933** | 109.5 | 2.5 hours |
| **LoRA** | | | | | | |
| E5 + LoRA (IMDB) | 89.8% | 0.897 | 0.899 | 0.895 | 0.3 | 40 min |
| E5 + LoRA (Yelp) | 89.5% | 0.894 | 0.896 | 0.892 | 0.3 | 40 min |
| E5 + LoRA (Fusion) | 91.1% | 0.910 | 0.912 | 0.908 | 0.3 | 1.5 hours |

#### Cross-Domain Performance (Amazon Test Set)

| Model | Accuracy | F1 Score | Performance Drop | Generalization |
|-------|----------|----------|------------------|----------------|
| **Fine-Tuning** | | | | |
| TF-IDF + FFNN | 82.3% | 0.821 | -6.2% | ‚≠ê‚≠ê‚≠ê |
| E5 Classifier | 85.7% | 0.856 | -4.5% | ‚≠ê‚≠ê‚≠ê‚≠ê |
| BERT (Full) | 88.9% | 0.888 | -4.5% | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **LoRA** | | | | |
| E5 + LoRA (Fusion) | 87.2% | 0.871 | -3.9% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Key Findings**:
1. ‚úÖ BERT full fine-tuning achieves highest in-domain accuracy
2. ‚úÖ LoRA Fusion shows **best generalization** (smallest performance drop)
3. ‚úÖ LoRA trains **4-6x faster** than full BERT fine-tuning
4. ‚úÖ LoRA uses **< 1% parameters** of full fine-tuning

### Efficiency Comparison

| Metric | BERT Full Fine-Tuning | E5 + LoRA Fusion | Advantage |
|--------|----------------------|------------------|-----------|
| **Trainable Parameters** | 109.5M (100%) | 0.3M (< 1%) | **365x fewer** |
| **Model Size** | 440 MB | 1.2 MB per adapter | **366x smaller** |
| **Training Time** | 2.5 hours | 1.5 hours | **1.7x faster** |
| **GPU Memory** | ~12 GB | ~4 GB | **3x less** |
| **In-Domain F1** | 0.934 | 0.910 | -2.4% |
| **Cross-Domain F1** | 0.888 | 0.871 | -1.7% |

**Conclusion**: LoRA achieves **~97% of full fine-tuning performance** with **< 1% of parameters** and **significantly faster training**.

---

## üìù For Academic Submission

### What to Include in Your Report

#### 1. **Code Repository** (This README + Code)
- ‚úÖ Complete implementation
- ‚úÖ Reproducible experiments
- ‚úÖ Clear documentation
- ‚úÖ Installation instructions

#### 2. **Experimental Setup Section**
```markdown
### Experimental Setup

**Dataset**: 
- IMDB (movie reviews): 24k train, 6k eval, 6k test
- Yelp (restaurant reviews): 24k train, 6k eval, 6k test  
- Amazon (product reviews): 12k test (cross-domain)

**Models**:
- Method A: TF-IDF+FFNN, E5 Classifier, BERT full fine-tuning
- Method B: E5 + LoRA domain adapters with fusion

**Optimization**:
- K-Fold Cross-Validation (2-3 folds)
- Optuna hyperparameter search (8-15 trials)
- Early stopping (patience: 2-3 epochs)

**Metrics**:
- Accuracy, F1-Score, Precision, Recall
- Cross-domain performance (domain adaptation)
- Training time, parameter efficiency

**Hardware**: 
- GPU: NVIDIA [Your GPU Model]
- CPU: [Your CPU]
- RAM: [Your RAM]
```

#### 3. **Results Tables** (From third.ipynb)
- Copy tables and figures from notebook outputs
- Include: `outputs/comparison/model_comparison.csv`
- Include: `outputs/comparison/*.png` figures

#### 4. **Discussion Points**

**Performance**:
- "BERT full fine-tuning achieves highest accuracy (93.4%) but requires 109M parameters"
- "LoRA Fusion achieves competitive performance (91.1%) with only 0.3M parameters"
- "LoRA shows better generalization on cross-domain data"

**Efficiency**:
- "LoRA reduces trainable parameters by 365x"
- "Training time reduced by 1.7x"
- "Model storage reduced by 366x"

**Trade-offs**:
- "Small performance gap (2.4% F1) acceptable for massive efficiency gains"
- "LoRA ideal for resource-constrained deployment"
- "Full fine-tuning preferred when maximum accuracy is critical"

---

### Files to Clean Before Submission

#### ‚ùå Remove These (Development Files)

```bash
# In fine_tuning/
rm -rf __pycache__/
rm -rf .ipynb_checkpoints/
rm -f *.log
rm -f training_*.log
rm -rf outputs/logs/  # Keep only final results

# In lora/
rm -rf __pycache__/
rm -f *.log
rm -rf outputs/logs/  # Keep only final models

# In data/processed/
# Keep only: train.json, eval.json, *_eval.json, dataset_summary.json
rm -f *.npy  # Remove any numpy cache files
rm -f *.pkl  # Remove any pickle cache files

# Root level
rm -f .DS_Store
rm -rf .git/  # If you want to submit as zip without git history
```

#### ‚úÖ Keep These (Essential Files)

```bash
export_package/
‚îú‚îÄ‚îÄ README.md                        # ‚úÖ This file
‚îú‚îÄ‚îÄ requirements.txt                 # ‚úÖ Dependencies
‚îú‚îÄ‚îÄ data/processed/                  # ‚úÖ Datasets
‚îÇ   ‚îú‚îÄ‚îÄ train.json
‚îÇ   ‚îú‚îÄ‚îÄ eval.json
‚îÇ   ‚îî‚îÄ‚îÄ *_eval.json
‚îú‚îÄ‚îÄ fine_tuning/
‚îÇ   ‚îú‚îÄ‚îÄ first.ipynb                  # ‚úÖ Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ train_all_models.py          # ‚úÖ Training script
‚îÇ   ‚îú‚îÄ‚îÄ third.ipynb                  # ‚úÖ Evaluation
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING_README.md           # ‚úÖ Documentation
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # ‚úÖ Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ models/                      # ‚úÖ Best models only
‚îÇ       ‚îú‚îÄ‚îÄ tfidf_ffnn/best_model.pt
‚îÇ       ‚îú‚îÄ‚îÄ e5_classifier/best_model.pt
‚îÇ       ‚îî‚îÄ‚îÄ bert_finetuned/final_model/
‚îî‚îÄ‚îÄ lora/
    ‚îú‚îÄ‚îÄ train_method_b_e5.py         # ‚úÖ LoRA training
    ‚îú‚îÄ‚îÄ models_e5.py                 # ‚úÖ Model definitions
    ‚îú‚îÄ‚îÄ metrics_utils.py             # ‚úÖ Utilities
    ‚îî‚îÄ‚îÄ outputs/                     # ‚úÖ Best adapters only
        ‚îú‚îÄ‚îÄ domain_adapter_imdb.pt
        ‚îú‚îÄ‚îÄ domain_adapter_yelp.pt
        ‚îî‚îÄ‚îÄ fusion_model.pt
```

#### Cleaning Script

```bash
#!/bin/bash
# clean_for_submission.sh

echo "Cleaning development files for submission..."

# Remove Python cache
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null
find . -type d -name ".ipynb_checkpoints" -exec rm -rf {} + 2>/dev/null

# Remove logs
find . -type f -name "*.log" -delete
find . -type d -name "logs" -exec rm -rf {} + 2>/dev/null

# Remove temporary files
find . -type f -name ".DS_Store" -delete
find . -type f -name "*.pyc" -delete

# Remove large intermediate checkpoints (keep only best models)
cd fine_tuning/models/
# Keep only best_model.pt and final_model/
find . -type d -name "checkpoint-*" -exec rm -rf {} + 2>/dev/null
find . -type f -name "training_history.pkl" -delete 2>/dev/null

cd ../../lora/outputs/
# Keep only final adapters
find . -type f -name "*trial*" -delete 2>/dev/null
find . -type d -name "trial_*" -exec rm -rf {} + 2>/dev/null

cd ../..

echo "‚úÖ Cleaning complete!"
echo "Project is now ready for submission."
```

Make it executable and run:
```bash
chmod +x clean_for_submission.sh
./clean_for_submission.sh
```

---

### Submission Checklist

- [ ] Code runs without errors
- [ ] All notebooks execute from top to bottom
- [ ] Results are reproducible (with seed=42)
- [ ] README is complete and accurate
- [ ] Requirements.txt is up to date
- [ ] Comments are clear and professional
- [ ] No personal/sensitive information in code
- [ ] Large checkpoint files removed
- [ ] Only essential models kept
- [ ] Folder structure is clean
- [ ] File naming is consistent
- [ ] No debug print statements
- [ ] License file included (if required)

### Creating Submission Archive

```bash
# Create clean copy
cd /path/to/FYP-Research
cp -r export_package submission_package
cd submission_package

# Run cleaning script
bash clean_for_submission.sh

# Create archive
cd ..
tar -czf SC4001_FinetuningVsLoRA_[YourName].tar.gz submission_package/
# or
zip -r SC4001_FinetuningVsLoRA_[YourName].zip submission_package/

# Verify size (should be < 100MB without large model files)
du -sh SC4001_FinetuningVsLoRA_[YourName].tar.gz
```

---

## üìñ Citation

If you use this code or methodology in your research, please cite:

```bibtex
@misc{sentiment_finetuning_lora_2025,
  title={Fine-Tuning vs LoRA: A Comparative Study for Sentiment Analysis with Domain Adaptation},
  author={[Your Names]},
  year={2025},
  school={Nanyang Technological University},
  course={SC4001: Neural Networks and Deep Learning},
  note={Comparison of traditional fine-tuning and parameter-efficient LoRA for multi-domain sentiment analysis}
}
```

### References

**Key Papers**:
1. **LoRA**: Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models", ICLR 2022
2. **BERT**: Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers", NAACL 2019
3. **E5**: Wang et al., "Text Embeddings by Weakly-Supervised Contrastive Pre-training", 2022
4. **Domain Adaptation**: Ben-David et al., "A theory of learning from different domains", Machine Learning 2010

**Datasets**:
- IMDB: Maas et al., "Learning Word Vectors for Sentiment Analysis", ACL 2011
- Yelp: Yelp Dataset Challenge
- Amazon: McAuley et al., "Ups and Downs: Modeling the Visual Evolution of Fashion Trends", WWW 2016

---

## üë• Contributors

**Project Team**:
- [Student Name 1] - [Email/GitHub]
- [Student Name 2] - [Email/GitHub]
- [Student Name 3] - [Email/GitHub]

**Course**: SC4001 - Neural Networks and Deep Learning  
**Institution**: Nanyang Technological University  
**Academic Year**: 2024/2025  
**Submission Date**: [Your Date]

---

## üìÑ License

This project is submitted as part of academic coursework for SC4001 at NTU.

For academic and educational purposes only. Not for commercial use.

---

## üôè Acknowledgments

- **Course Instructor**: [Professor Name]
- **Teaching Assistants**: [TA Names]
- **Hugging Face**: For providing transformer models and PEFT library
- **PyTorch Team**: For the deep learning framework
- **Optuna**: For hyperparameter optimization framework

---

## üìû Support & Contact

For questions regarding this project:

- **Email**: [your.email@university.edu]
- **Course Forum**: [Link to course forum]
- **Office Hours**: [Schedule]

---

**Last Updated**: November 2025

---

